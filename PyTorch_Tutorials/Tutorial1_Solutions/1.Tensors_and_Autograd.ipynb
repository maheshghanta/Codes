{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensors and Autograd Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maheshghanta/Codes/blob/master/PyTorch_Tutorials/Tutorial1_Solutions/1.Tensors_and_Autograd.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0aa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install ipywidgets\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: Scalars, Vectors, and Tensors\n",
    "\n",
    "### **Scalar**\n",
    "- Single value (0D): `5`\n",
    "- Use: loss, learning rate\n",
    "\n",
    "### **Vector**\n",
    "- 1D array: `[1,2,3]`\n",
    "- Use: embeddings, features\n",
    "\n",
    "### **Matrix**\n",
    "- 2D array: `[[1,2],[3,4]]`\n",
    "- Use: weights, batch data\n",
    "\n",
    "### **Tensor**\n",
    "- ND array: generalizes all above\n",
    "- 4D example: `(batch, channels, height, width)`\n",
    "- Use: images, video, any ND data\n",
    "\n",
    "**In PyTorch, everything is a tensor!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations: PyTorch vs NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From lists\n",
    "np_arr = np.array([1,2,3])\n",
    "torch_t = torch.tensor([1,2,3])\n",
    "print(\"NumPy:\", np_arr)\n",
    "print(\"PyTorch:\", torch_t)\n",
    "\n",
    "# Zeros, ones, random\n",
    "print(\"\\nZeros:\", torch.zeros(2,3).shape)\n",
    "print(\"Ones:\", torch.ones(2,3).shape)\n",
    "print(\"Random:\", torch.randn(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NumPy ↔ PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_a = np.array([[1,2],[3,4]])\n",
    "torch_a = torch.from_numpy(np_a)\n",
    "print(\"NumPy→PyTorch:\", torch_a)\n",
    "print(torch_a.dtype)\n",
    "torch_b = torch.tensor([[5,6],[7,8]])\n",
    "np_b = torch_b.numpy()\n",
    "print(\"PyTorch→NumPy:\", np_b)\n",
    "print(np_b.dtype)\n",
    "\n",
    "# They share memory!\n",
    "np_a[0,0] = 999\n",
    "print(\"Modified NumPy affects PyTorch:\", torch_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(\"Add:\", a + b)\n",
    "print(\"Multiply:\", a * b)\n",
    "print(\"Matmul:\", torch.matmul(a, b))\n",
    "print(\"Transpose:\", a.T)\n",
    "print(\"Sum:\", a.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(12)\n",
    "print(\"Original:\", t.shape)\n",
    "print(\"Reshaped 3x4:\\n\", t.reshape(3,4))\n",
    "print(\"View 2x6:\\n\", t.view(2,6))\n",
    "print(\"Index [0,1]:\", t.reshape(3,4)[0,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Performance (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "size = 1000\n",
    "x_cpu = torch.randn(size, size)\n",
    "y_cpu = torch.randn(size, size)\n",
    "\n",
    "start = time.time()\n",
    "result = torch.matmul(x_cpu, y_cpu)\n",
    "print(f\"CPU time: {time.time()-start:.4f}s\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.to(device)\n",
    "    y_gpu = y_cpu.to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    result_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU time: {time.time()-start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Backpropagation\n",
    "\n",
    "### Function: $f(x,y) = x^2 + 2xy + y^2$\n",
    "\n",
    "**Derivatives:**\n",
    "- $\\\\frac{\\\\partial f}{\\\\partial x} = 2x + 2y$\n",
    "- $\\\\frac{\\\\partial f}{\\\\partial y} = 2x + 2y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d725a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare meshgrid for x and y in reasonable range\n",
    "x_vals = np.linspace(0, 6, 100)\n",
    "y_vals = np.linspace(0, 6, 100)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "F = X**2 + 2*X*Y + Y**2  # The function f = x**2 + 2*x*y + y**2\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, F, cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "ax.set_title(r\"$f(x, y) = x^2 + 2xy + y^2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y):\n",
    "    return x**2 + 2*x*y + y**2\n",
    "\n",
    "def backward(x, y):\n",
    "    return 2*x + 2*y, 2*x + 2*y\n",
    "\n",
    "x, y = 3.0, 4.0\n",
    "out = forward(x, y)\n",
    "gx, gy = backward(x, y)\n",
    "\n",
    "print(f\"f({x},{y}) = {out}\")\n",
    "print(f\"∂f/∂x = {gx}\")\n",
    "print(f\"∂f/∂y = {gy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Example: $y = \\\\sigma(Wx + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def sigmoid_grad(z): s=sigmoid(z); return s*(1-s)\n",
    "\n",
    "W = np.array([[0.5,-0.3],[0.2,0.8]])\n",
    "b = np.array([0.1,-0.2])\n",
    "x = np.array([1.0,2.0])\n",
    "\n",
    "# Forward\n",
    "z = W @ x + b\n",
    "y = sigmoid(z)\n",
    "print(\"Forward:\", y)\n",
    "\n",
    "# Backward\n",
    "grad_z = sigmoid_grad(z)\n",
    "grad_W = np.outer(grad_z, x)\n",
    "grad_b = grad_z\n",
    "grad_x = W.T @ grad_z\n",
    "print(\"∂L/∂W:\\n\", grad_W)\n",
    "print(\"∂L/∂b:\", grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Autograd\n",
    "\n",
    "Automatic differentiation - no manual gradient calculation needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "f = x**2 + 2*x*y + y**2\n",
    "print(f\"f = {f.item()}\")\n",
    "\n",
    "f.backward()\n",
    "print(f\"∂f/∂x = {x.grad.item()}\")\n",
    "print(f\"∂f/∂y = {y.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Neural Network Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[0.5,-0.3],[0.2,0.8]], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([0.1,-0.2], requires_grad=True, dtype=torch.float32)\n",
    "x = torch.tensor([1.0,2.0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "z = torch.matmul(W, x) + b\n",
    "y = torch.sigmoid(z)\n",
    "loss = y.sum()\n",
    "\n",
    "loss.backward()\n",
    "print(\"∂L/∂W:\\n\", W.grad.numpy())\n",
    "print(\"∂L/∂b:\", b.grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba341b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # W and b are encapsulated here\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "simple_model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624046",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = f'runs/simple_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "writer = SummaryWriter(run_dir)\n",
    "print(f\"TensorBoard logs saved to: {run_dir}\")\n",
    "print(f\"View with: tensorboard --logdir=runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input tensor and add graph\n",
    "x = torch.tensor([1.0,2.0], requires_grad=True, dtype=torch.float32)\n",
    "# Add graph only once - this creates the computation graph visualization\n",
    "writer.add_graph(simple_model, x)\n",
    "writer.close()\n",
    "print(\"Graph added successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Autograd Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation\n",
    "print(\"1. Accumulation:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for i in range(3):\n",
    "    (x**2).backward()\n",
    "    print(f\"  Iter {i+1}: grad = {x.grad.item()}\")\n",
    "print(\"  Gradients accumulate!\\n\")\n",
    "\n",
    "# Zero gradients\n",
    "x.grad.zero_()\n",
    "print(\"2. After zeroing:\", x.grad.item())\n",
    "\n",
    "# Detach\n",
    "print(\"\\n3. Detach:\")\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = y.detach()\n",
    "print(f\"  y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"  z.requires_grad: {z.requires_grad}\")\n",
    "\n",
    "# No grad context\n",
    "print(\"\\n4. No grad (inference):\")\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(f\"  requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: PyTorch NN Layers\n",
    "\n",
    "### Linear\n",
    "- `nn.Linear(in, out)` - Fully connected\n",
    "- `nn.Bilinear()` - Bilinear transformation\n",
    "\n",
    "### Convolutional\n",
    "- `nn.Conv1d/2d/3d()` - 1D/2D/3D convolution\n",
    "- `nn.ConvTranspose2d()` - Upsampling\n",
    "\n",
    "### Pooling\n",
    "- `nn.MaxPool2d()` - Max pooling\n",
    "- `nn.AvgPool2d()` - Average pooling\n",
    "- `nn.AdaptiveAvgPool2d()` - Adaptive pooling\n",
    "\n",
    "### Activation\n",
    "- `nn.ReLU()`, `nn.LeakyReLU()`, `nn.GELU()`\n",
    "- `nn.Sigmoid()`, `nn.Tanh()`, `nn.Softmax()`\n",
    "\n",
    "### Normalization\n",
    "- `nn.BatchNorm2d()` - Batch normalization\n",
    "- `nn.LayerNorm()` - Layer normalization\n",
    "- `nn.GroupNorm()` - Group normalization\n",
    "\n",
    "### Recurrent\n",
    "- `nn.RNN()`, `nn.LSTM()`, `nn.GRU()`\n",
    "\n",
    "### Transformer\n",
    "- `nn.Transformer()` - Full transformer\n",
    "- `nn.TransformerEncoder/Decoder()`\n",
    "- `nn.MultiheadAttention()`\n",
    "\n",
    "### Regularization\n",
    "- `nn.Dropout()`, `nn.Dropout2d()`\n",
    "\n",
    "### Embedding\n",
    "- `nn.Embedding()` - Lookup table\n",
    "\n",
    "### Loss Functions\n",
    "- `nn.CrossEntropyLoss()` - Classification\n",
    "- `nn.MSELoss()` - Regression\n",
    "- `nn.BCEWithLogitsLoss()` - Binary classification\n",
    "\n",
    "### Utility\n",
    "- `nn.Sequential()` - Chain layers\n",
    "- `nn.ModuleList/Dict()` - Dynamic layers\n",
    "- `nn.Flatten()` - Flatten dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12393453",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True)\n",
    "image, label = image_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique run directory with timestamp to avoid multiple graph events\n",
    "run_dir = f'runs/simple_cnn_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "writer = SummaryWriter(run_dir)\n",
    "print(f\"TensorBoard logs saved to: {run_dir}\")\n",
    "\n",
    "# Convert CIFAR10 image to correct PyTorch format\n",
    "# CIFAR10 images are (H, W, C) format, but PyTorch CNNs need (B, C, H, W)\n",
    "x = np.asarray(image)  # Shape: (32, 32, 3)\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "\n",
    "# Use .permute() to rearrange dimensions from (H, W, C) to (C, H, W)\n",
    "x = x.permute(2, 0, 1)  # Now shape: (3, 32, 32)\n",
    "print(f\"After permute: {x.shape}\")\n",
    "\n",
    "# Add batch dimension\n",
    "x = x.unsqueeze(0)  # Now shape: (1, 3, 32, 32)\n",
    "x.requires_grad = True\n",
    "print(f\"Final shape: {x.shape}\")\n",
    "\n",
    "# Add graph only once\n",
    "writer.add_graph(model, x)\n",
    "writer.close()\n",
    "print(\"CNN graph added successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
