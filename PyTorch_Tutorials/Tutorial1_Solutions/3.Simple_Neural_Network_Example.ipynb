{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a873b30",
   "metadata": {},
   "source": [
    "# Simple Neural Network for Image Classification\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maheshghanta/Codes/blob/master/PyTorch_Tutorials/Tutorial1_Solutions/3.Simple_Neural_Network_Example.ipynb)\n",
    "\n",
    "This tutorial demonstrates building a **fully connected neural network** (no CNN) for CIFAR-10 image classification with:\n",
    "- Complete training pipeline\n",
    "- TensorBoard logging\n",
    "- Metrics visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892cb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install ipywidgets\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956302bc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We'll build a **Multi-Layer Perceptron (MLP)** - a fully connected neural network - to classify CIFAR-10 images.\n",
    "\n",
    "**Key Components:**\n",
    "- Input: Flattened 32×32×3 images (3072 features)\n",
    "- Hidden layers: Fully connected layers with ReLU activation\n",
    "- Output: 10 classes (CIFAR-10 categories)\n",
    "- Training: Backpropagation with cross-entropy loss\n",
    "- Logging: TensorBoard for monitoring training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db0265",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6190de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f265",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a22fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "# We'll convert images to tensors and normalize them\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image to tensor [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {test_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Check data shape\n",
    "sample_image, sample_label = next(iter(train_loader))\n",
    "print(f\"Batch shape: {sample_image.shape}\")  # (batch_size, channels, height, width)\n",
    "print(f\"Label shape: {sample_label.shape}\")\n",
    "print(f\"Image size per sample: {sample_image[0].shape}\")\n",
    "print(f\"Flattened size: {sample_image[0].numel()} = 3*32*32 = 3072\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f0300",
   "metadata": {},
   "source": [
    "## 2. Define Neural Network Model\n",
    "\n",
    "We'll create a Multi-Layer Perceptron (MLP) - a fully connected neural network without convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi-Layer Perceptron for CIFAR-10 classification\n",
    "    Architecture: Input(3072) -> Hidden1(512) -> Hidden2(256) -> Output(10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=3072, hidden1_size=512, hidden2_size=256, num_classes=10):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten input: (batch_size, 3, 32, 32) -> (batch_size, 3072)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation, will use CrossEntropyLoss which includes softmax)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleMLP()\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a341e",
   "metadata": {},
   "source": [
    "## 3. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (CrossEntropyLoss includes softmax)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d5764",
   "metadata": {},
   "source": [
    "## 4. Setup TensorBoard Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard writer with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f'runs/mlp_cifar10_{timestamp}'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "print(f\"To view: tensorboard --logdir=runs\")\n",
    "\n",
    "# Log model architecture\n",
    "sample_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "writer.add_graph(model, sample_input)\n",
    "print(\"Model graph added to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df2c42",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35265469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device, epoch, writer):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    \n",
    "    Returns:\n",
    "        average_loss: Average training loss\n",
    "        accuracy: Training accuracy (%)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(loader):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Log batch metrics to TensorBoard\n",
    "        if batch_idx % 100 == 0:\n",
    "            writer.add_scalar('Train/BatchLoss', loss.item(), epoch * len(loader) + batch_idx)\n",
    "            batch_acc = 100. * correct / total\n",
    "            writer.add_scalar('Train/BatchAccuracy', batch_acc, epoch * len(loader) + batch_idx)\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device, epoch, writer, phase='Validation'):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \n",
    "    Returns:\n",
    "        average_loss: Average validation loss\n",
    "        accuracy: Validation accuracy (%)\n",
    "        all_preds: All predictions\n",
    "        all_labels: All true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Log epoch metrics to TensorBoard\n",
    "    writer.add_scalar(f'{phase}/Loss', avg_loss, epoch)\n",
    "    writer.add_scalar(f'{phase}/Accuracy', accuracy, epoch)\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ed9d4",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Store metrics for plotting\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, epoch, writer\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(\n",
    "        model, val_loader, criterion, device, epoch, writer, 'Validation'\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar('Train/LearningRate', current_lr, epoch)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {current_lr}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"✓ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34635aab",
   "metadata": {},
   "source": [
    "## 7. Plot Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(1, num_epochs+1), train_losses, 'b-', label='Train Loss', marker='o')\n",
    "axes[0].plot(range(1, num_epochs+1), val_losses, 'r-', label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(range(1, num_epochs+1), train_accs, 'b-', label='Train Acc', marker='o')\n",
    "axes[1].plot(range(1, num_epochs+1), val_accs, 'r-', label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f6aaf",
   "metadata": {},
   "source": [
    "## 8. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = validate(\n",
    "    model, test_loader, criterion, device, 0, writer, 'Test'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7eda6c",
   "metadata": {},
   "source": [
    "## 9. Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ca2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "from collections import defaultdict\n",
    "\n",
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)\n",
    "\n",
    "for pred, label in zip(test_preds, test_labels):\n",
    "    if pred == label:\n",
    "        class_correct[label] += 1\n",
    "    class_total[label] += 1\n",
    "\n",
    "classes = test_dataset.classes\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"-\" * 40)\n",
    "class_accs = []\n",
    "for i, class_name in enumerate(classes):\n",
    "    acc = 100.0 * class_correct[i] / class_total[i]\n",
    "    class_accs.append(acc)\n",
    "    print(f\"{class_name:12s}: {acc:6.2f}% ({class_correct[i]}/{class_total[i]})\")\n",
    "\n",
    "# Plot per-class accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(classes)), class_accs, color='skyblue', edgecolor='navy')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Accuracy on Test Set')\n",
    "plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
    "plt.ylim([0, 100])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, class_accs):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPer-class accuracy plot saved to 'per_class_accuracy.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b16d9",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, yticklabels=classes,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202bcd3",
   "metadata": {},
   "source": [
    "## 11. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c88835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "model.eval()\n",
    "test_images, test_labels_batch = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_images)\n",
    "    _, predictions = outputs.max(1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_images = test_images.cpu()\n",
    "predictions = predictions.cpu()\n",
    "\n",
    "# Visualize first 16 predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Denormalize image\n",
    "    img = test_images[i].permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Color: green if correct, red if wrong\n",
    "    true_label = classes[test_labels_batch[i]]\n",
    "    pred_label = classes[predictions[i]]\n",
    "    color = 'green' if test_labels_batch[i] == predictions[i] else 'red'\n",
    "    \n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}', color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions (Green=Correct, Red=Wrong)', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample predictions saved to 'sample_predictions.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
