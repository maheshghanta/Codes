{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensors and Autograd Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maheshghanta/Codes/blob/master/PyTorch_Tutorials/Tutorial1_Exercises/1.Tensors_and_Autograd_Exercise.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349407c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install ipywidgets\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install scipy\n",
    "%pip install -i https://test.pypi.org/simple/ exercise-validation==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dec4f",
   "metadata": {},
   "source": [
    "## Overview: Scalars, Vectors, and Tensors\n",
    "\n",
    "### **Scalar**\n",
    "- Single value (0D): `5`\n",
    "- Use: loss, learning rate\n",
    "\n",
    "### **Vector**\n",
    "- 1D array: `[1,2,3]`\n",
    "- Use: embeddings, features\n",
    "\n",
    "### **Matrix**\n",
    "- 2D array: `[[1,2],[3,4]]`\n",
    "- Use: weights, batch data\n",
    "\n",
    "### **Tensor**\n",
    "- ND array: generalizes all above\n",
    "- 4D example: `(batch, channels, height, width)`\n",
    "- Use: images, video, any ND data\n",
    "\n",
    "**In PyTorch, everything is a tensor!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99635ac",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98382dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from exercise_validation import (\n",
    "    validate_exercise_1,\n",
    "    validate_exercise_2,\n",
    "    validate_exercise_3,\n",
    "    validate_exercise_4,\n",
    "    validate_exercise_5,\n",
    "    validate_exercise_6,\n",
    "    validate_exercise_7,\n",
    "    validate_exercise_8\n",
    ")\n",
    "\n",
    "from exercise_validation import create_feedback, create_check_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52784ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbbf4c",
   "metadata": {},
   "source": [
    "## ðŸŽ® Exercise Validation Framework\n",
    "\n",
    "Run this cell to load the validation system for interactive exercises!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9223f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_variable_exists(var_name, local_vars):\n",
    "    \"\"\"Check if a variable exists in the local scope\"\"\"\n",
    "    if var_name not in local_vars:\n",
    "        return False, f\"Variable '{var_name}' not found. Did you define it?\"\n",
    "    return True, \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd256ade",
   "metadata": {},
   "source": [
    "## Tensor Operations: PyTorch vs NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad4403",
   "metadata": {},
   "source": [
    "### 1. Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a612e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From lists\n",
    "np_arr = np.array([1,2,3])\n",
    "torch_t = torch.tensor([1,2,3])\n",
    "print(\"NumPy:\", np_arr)\n",
    "print(\"PyTorch:\", torch_t)\n",
    "\n",
    "# Zeros, ones, random\n",
    "print(\"\\nZeros:\", torch.zeros(2,3).shape)\n",
    "print(\"Ones:\", torch.ones(2,3).shape)\n",
    "print(\"Random:\", torch.randn(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b9bd4",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 1: Creating Tensors\n",
    "\n",
    "**Your Task:** Fill in the blanks to create the specified tensors.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create a tensor of zeros with shape (3, 4)\n",
    "2. Create a tensor of ones with shape (2, 5)\n",
    "3. Create a random tensor with shape (3, 3)\n",
    "\n",
    "Replace the `None` values with the correct PyTorch function calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Create a tensor of zeros with shape (3, 4)\n",
    "zeros_tensor = None  # Replace None with torch.zeros(...)\n",
    "\n",
    "# Create a tensor of ones with shape (2, 5)\n",
    "ones_tensor = None  # Replace None with torch.ones(...)\n",
    "\n",
    "# Create a random tensor with shape (3, 3)\n",
    "random_tensor = None  # Replace None with torch.randn(...)\n",
    "\n",
    "# Don't modify below - this displays your results\n",
    "if zeros_tensor is not None:\n",
    "    print(\"Zeros tensor shape:\", zeros_tensor.shape)\n",
    "if ones_tensor is not None:\n",
    "    print(\"Ones tensor shape:\", ones_tensor.shape)\n",
    "if random_tensor is not None:\n",
    "    print(\"Random tensor shape:\", random_tensor.shape)\n",
    "    \n",
    "create_check_button(\"exercise_1\", lambda: validate_exercise_1(zeros_tensor, ones_tensor, random_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d96a13",
   "metadata": {},
   "source": [
    "### 2. NumPy â†” PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_a = np.array([[1,2],[3,4]])\n",
    "torch_a = torch.from_numpy(np_a)\n",
    "print(\"NumPyâ†’PyTorch:\", torch_a)\n",
    "print(torch_a.dtype)\n",
    "torch_b = torch.tensor([[5,6],[7,8]])\n",
    "np_b = torch_b.numpy()\n",
    "print(\"PyTorchâ†’NumPy:\", np_b)\n",
    "print(np_b.dtype)\n",
    "\n",
    "# They share memory!\n",
    "np_a[0,0] = 999\n",
    "print(\"Modified NumPy affects PyTorch:\", torch_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8001f2c",
   "metadata": {},
   "source": [
    "### 3. Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8dec82",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 2: NumPy â†” PyTorch Conversion\n",
    "\n",
    "**Your Task:** Convert between NumPy arrays and PyTorch tensors.\n",
    "\n",
    "**Instructions:**\n",
    "1. Convert a NumPy array to a PyTorch tensor\n",
    "2. Convert a PyTorch tensor to a NumPy array\n",
    "3. Verify the conversion worked correctly\n",
    "\n",
    "Replace the `None` values with the correct conversion functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83837c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Given NumPy array\n",
    "numpy_array = np.array([[10, 20, 30], [40, 50, 60]])\n",
    "print(\"Original NumPy array:\")\n",
    "print(numpy_array)\n",
    "\n",
    "# Convert NumPy to PyTorch tensor\n",
    "torch_from_numpy = None  # Replace None with the conversion function\n",
    "\n",
    "# Given PyTorch tensor\n",
    "torch_tensor = torch.tensor([[1.5, 2.5], [3.5, 4.5]])\n",
    "print(\"\\nOriginal PyTorch tensor:\")\n",
    "print(torch_tensor)\n",
    "\n",
    "# Convert PyTorch to NumPy array\n",
    "numpy_from_torch = None  # Replace None with the conversion function\n",
    "\n",
    "# Don't modify below - this displays your results\n",
    "if torch_from_numpy is not None:\n",
    "    print(\"\\nConverted to PyTorch:\", torch_from_numpy)\n",
    "if numpy_from_torch is not None:\n",
    "    print(\"Converted to NumPy:\", numpy_from_torch)\n",
    "\n",
    "#Validation for Exercise 2\n",
    "create_check_button(\"exercise_2\", lambda: validate_exercise_2(torch_from_numpy, numpy_from_torch, numpy_array, torch_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "print(\"Add:\", a + b)\n",
    "print(\"Multiply:\", a * b)\n",
    "print(\"Matmul:\", torch.matmul(a, b))\n",
    "print(\"Transpose:\", a.T)\n",
    "print(\"Sum:\", a.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479cd3fe",
   "metadata": {},
   "source": [
    "### 4. Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(12)\n",
    "print(\"Original:\", t.shape)\n",
    "print(\"Reshaped 3x4:\\n\", t.reshape(3,4))\n",
    "print(\"View 2x6:\\n\", t.view(2,6))\n",
    "print(\"Index [0,1]:\", t.reshape(3,4)[0,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1db02b",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 3: Tensor Operations\n",
    "\n",
    "**Your Task:** Perform basic tensor operations.\n",
    "\n",
    "**Instructions:**\n",
    "Given two tensors `matrix_a` and `matrix_b`, compute:\n",
    "1. Element-wise multiplication\n",
    "2. Matrix multiplication (matmul)\n",
    "3. Transpose of matrix_a\n",
    "4. Sum of all elements in matrix_a\n",
    "\n",
    "Replace the `None` values with the correct operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fa125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Given matrices\n",
    "matrix_a = torch.tensor([[2, 4], [6, 8]])\n",
    "matrix_b = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(matrix_a)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(matrix_b)\n",
    "\n",
    "# Perform operations\n",
    "elementwise_mult = None  # Element-wise multiplication: matrix_a * matrix_b\n",
    "matrix_mult = None  # Matrix multiplication: use torch.matmul(matrix_a, matrix_b)\n",
    "transposed = None  # Transpose of matrix_a: use matrix_a.T\n",
    "total_sum = None  # Sum of all elements in matrix_a: use matrix_a.sum()\n",
    "\n",
    "# Don't modify below - this displays your results\n",
    "print(\"\\n--- Your Results ---\")\n",
    "if elementwise_mult is not None:\n",
    "    print(\"Element-wise multiplication:\\n\", elementwise_mult)\n",
    "if matrix_mult is not None:\n",
    "    print(\"\\nMatrix multiplication:\\n\", matrix_mult)\n",
    "if transposed is not None:\n",
    "    print(\"\\nTranspose:\\n\", transposed)\n",
    "if total_sum is not None:\n",
    "    print(\"\\nSum:\", total_sum.item() if isinstance(total_sum, torch.Tensor) else total_sum)\n",
    "\n",
    "create_check_button(\"exercise_3\", lambda: validate_exercise_3(elementwise_mult, matrix_mult, transposed, total_sum, matrix_a, matrix_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de660b5a",
   "metadata": {},
   "source": [
    "### 5. Performance (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f355bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "size = 1000\n",
    "x_cpu = torch.randn(size, size)\n",
    "y_cpu = torch.randn(size, size)\n",
    "\n",
    "start = time.time()\n",
    "result = torch.matmul(x_cpu, y_cpu)\n",
    "print(f\"CPU time: {time.time()-start:.4f}s\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.to(device)\n",
    "    y_gpu = y_cpu.to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    result_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"GPU time: {time.time()-start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c2144",
   "metadata": {},
   "source": [
    "## Manual Backpropagation\n",
    "\n",
    "### Function: $f(x,y) = x^2 + 2xy + y^2$\n",
    "\n",
    "**Derivatives:**\n",
    "- $\\\\frac{\\\\partial f}{\\\\partial x} = 2x + 2y$\n",
    "- $\\\\frac{\\\\partial f}{\\\\partial y} = 2x + 2y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4504b8",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 4: Reshaping Tensors\n",
    "\n",
    "**Your Task:** Practice reshaping tensors with different methods.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create a tensor with values from 0 to 23 using `torch.arange(24)`\n",
    "2. Reshape it to (4, 6) using `.reshape()`\n",
    "3. Reshape it to (2, 3, 4) using `.view()`\n",
    "4. Flatten it back to 1D using `.flatten()`\n",
    "\n",
    "Replace the `None` values with the correct operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb4eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Create a tensor with values from 0 to 23\n",
    "original_tensor = None  # Use torch.arange(24)\n",
    "\n",
    "# Reshape to (4, 6)\n",
    "reshaped_4x6 = None  # Use original_tensor.reshape(4, 6)\n",
    "\n",
    "# Reshape to (2, 3, 4)\n",
    "reshaped_2x3x4 = None  # Use original_tensor.view(2, 3, 4)\n",
    "\n",
    "# Flatten back to 1D\n",
    "flattened = None  # Use reshaped_2x3x4.flatten()\n",
    "\n",
    "# Don't modify below - this displays your results\n",
    "print(\"--- Your Results ---\")\n",
    "if original_tensor is not None:\n",
    "    print(\"Original shape:\", original_tensor.shape)\n",
    "if reshaped_4x6 is not None:\n",
    "    print(\"Reshaped to (4, 6):\", reshaped_4x6.shape)\n",
    "    print(reshaped_4x6)\n",
    "if reshaped_2x3x4 is not None:\n",
    "    print(\"\\nReshaped to (2, 3, 4):\", reshaped_2x3x4.shape)\n",
    "if flattened is not None:\n",
    "    print(\"Flattened:\", flattened.shape)\n",
    "\n",
    "# Validation for Exercise 4 (using exercise_validation package)\n",
    "def validate():\n",
    "    return \n",
    "\n",
    "create_check_button(\"exercise_4\", lambda: validate_exercise_4(original_tensor, reshaped_4x6, reshaped_2x3x4, flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d725a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare meshgrid for x and y in reasonable range\n",
    "x_vals = np.linspace(0, 6, 100)\n",
    "y_vals = np.linspace(0, 6, 100)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "F = X**2 + 2*X*Y + Y**2  # The function f = x**2 + 2*x*y + y**2\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, F, cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "ax.set_title(r\"$f(x, y) = x^2 + 2xy + y^2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y):\n",
    "    return x**2 + 2*x*y + y**2\n",
    "\n",
    "def backward(x, y):\n",
    "    return 2*x + 2*y, 2*x + 2*y\n",
    "\n",
    "x, y = 3.0, 4.0\n",
    "out = forward(x, y)\n",
    "gx, gy = backward(x, y)\n",
    "\n",
    "print(f\"f({x},{y}) = {out}\")\n",
    "print(f\"âˆ‚f/âˆ‚x = {gx}\")\n",
    "print(f\"âˆ‚f/âˆ‚y = {gy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b6f4d",
   "metadata": {},
   "source": [
    "### Complex Example: $y = \\\\sigma(Wx + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "def sigmoid_grad(z): s=sigmoid(z); return s*(1-s)\n",
    "\n",
    "W = np.array([[0.5,-0.3],[0.2,0.8]])\n",
    "b = np.array([0.1,-0.2])\n",
    "x = np.array([1.0,2.0])\n",
    "\n",
    "# Forward\n",
    "z = W @ x + b\n",
    "y = sigmoid(z)\n",
    "print(\"Forward:\", y)\n",
    "\n",
    "# Backward\n",
    "grad_z = sigmoid_grad(z)\n",
    "grad_W = np.outer(grad_z, x)\n",
    "grad_b = grad_z\n",
    "grad_x = W.T @ grad_z\n",
    "print(\"âˆ‚L/âˆ‚W:\\n\", grad_W)\n",
    "print(\"âˆ‚L/âˆ‚b:\", grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be3b65",
   "metadata": {},
   "source": [
    "## PyTorch Autograd\n",
    "\n",
    "Automatic differentiation - no manual gradient calculation needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90483b69",
   "metadata": {},
   "source": [
    "### 1. Simple Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d843eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "f = x**2 + 2*x*y + y**2\n",
    "print(f\"f = {f.item()}\")\n",
    "\n",
    "f.backward()\n",
    "print(f\"âˆ‚f/âˆ‚x = {x.grad.item()}\")\n",
    "print(f\"âˆ‚f/âˆ‚y = {y.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc03273",
   "metadata": {},
   "source": [
    "### 2. Neural Network Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf4cf3",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 5: Manual Gradient Calculation\n",
    "\n",
    "**Your Task:** Calculate gradients manually for a simple function.\n",
    "\n",
    "**Function:** $g(a, b) = 3a^2 + 2ab$\n",
    "\n",
    "**Derivatives:**\n",
    "- $\\\\frac{\\\\partial g}{\\\\partial a} = 6a + 2b$\n",
    "- $\\\\frac{\\\\partial g}{\\\\partial b} = 2a$\n",
    "\n",
    "**Instructions:**\n",
    "1. Implement the forward pass (calculate the function value)\n",
    "2. Implement the backward pass (calculate gradients manually)\n",
    "3. Test with $a = 2.0$, $b = 3.0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "def forward_pass(a, b):\n",
    "    \"\"\"Calculate g(a, b) = 3a^2 + 2ab\"\"\"\n",
    "    # Replace None with the correct formula\n",
    "    result = None  # Should be: 3*a**2 + 2*a*b\n",
    "    return result\n",
    "\n",
    "def backward_pass(a, b):\n",
    "    \"\"\"Calculate gradients: dg/da = 6a + 2b, dg/db = 2a\"\"\"\n",
    "    # Replace None with the correct gradients\n",
    "    grad_a = None  # Should be: 6*a + 2*b\n",
    "    grad_b = None  # Should be: 2*a\n",
    "    return grad_a, grad_b\n",
    "\n",
    "# Test values\n",
    "a, b = 2.0, 3.0\n",
    "\n",
    "# Calculate\n",
    "output = forward_pass(a, b)\n",
    "grad_a, grad_b = backward_pass(a, b)\n",
    "\n",
    "# Display results\n",
    "print(f\"Function value g({a}, {b}) = {output}\")\n",
    "print(f\"Gradient âˆ‚g/âˆ‚a = {grad_a}\")\n",
    "print(f\"Gradient âˆ‚g/âˆ‚b = {grad_b}\")\n",
    "\n",
    "create_check_button(\"exercise_5\", lambda: validate_exercise_5(forward_pass, backward_pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor([[0.5,-0.3],[0.2,0.8]], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([0.1,-0.2], requires_grad=True, dtype=torch.float32)\n",
    "x = torch.tensor([1.0,2.0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "z = torch.matmul(W, x) + b\n",
    "y = torch.sigmoid(z)\n",
    "loss = y.sum()\n",
    "\n",
    "loss.backward()\n",
    "print(\"âˆ‚L/âˆ‚W:\\n\", W.grad.numpy())\n",
    "print(\"âˆ‚L/âˆ‚b:\", b.grad.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba341b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # W and b are encapsulated here\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "simple_model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624046",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = f'runs/simple_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "writer = SummaryWriter(run_dir)\n",
    "print(f\"TensorBoard logs saved to: {run_dir}\")\n",
    "print(f\"View with: tensorboard --logdir=runs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input tensor and add graph\n",
    "x = torch.tensor([1.0,2.0], requires_grad=True, dtype=torch.float32)\n",
    "# Add graph only once - this creates the computation graph visualization\n",
    "writer.add_graph(simple_model, x)\n",
    "writer.close()\n",
    "print(\"Graph added successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36457c8a",
   "metadata": {},
   "source": [
    "### 3. Autograd Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfeccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient accumulation\n",
    "print(\"1. Accumulation:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for i in range(3):\n",
    "    (x**2).backward()\n",
    "    print(f\"  Iter {i+1}: grad = {x.grad.item()}\")\n",
    "print(\"  Gradients accumulate!\\n\")\n",
    "\n",
    "# Zero gradients\n",
    "x.grad.zero_()\n",
    "print(\"2. After zeroing:\", x.grad.item())\n",
    "\n",
    "# Detach\n",
    "print(\"\\n3. Detach:\")\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = y.detach()\n",
    "print(f\"  y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"  z.requires_grad: {z.requires_grad}\")\n",
    "\n",
    "# No grad context\n",
    "print(\"\\n4. No grad (inference):\")\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(f\"  requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aaba99",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 6: PyTorch Autograd\n",
    "\n",
    "**Your Task:** Use PyTorch's automatic differentiation to compute gradients.\n",
    "\n",
    "**Function:** $h(p, q) = p^3 + pq^2$\n",
    "\n",
    "**Instructions:**\n",
    "1. Create tensors `p` and `q` with `requires_grad=True`\n",
    "2. Compute the function `h`\n",
    "3. Call `.backward()` to compute gradients\n",
    "4. Access the gradients using `.grad`\n",
    "\n",
    "Test with $p = 2.0$, $q = 3.0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Create tensors with gradient tracking enabled\n",
    "p = None  # torch.tensor(2.0, requires_grad=True)\n",
    "q = None  # torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Compute the function h(p, q) = p^3 + p*q^2\n",
    "h = None  # p**3 + p * q**2\n",
    "\n",
    "# Compute gradients (call backward on h)\n",
    "# YOUR CODE HERE to call .backward()\n",
    "\n",
    "# Access gradients\n",
    "grad_p = None  # p.grad\n",
    "grad_q = None  # q.grad\n",
    "\n",
    "# Display results\n",
    "if h is not None:\n",
    "    print(f\"Function value h = {h.item()}\")\n",
    "if grad_p is not None:\n",
    "    print(f\"Gradient âˆ‚h/âˆ‚p = {grad_p.item()}\")\n",
    "if grad_q is not None:\n",
    "    print(f\"Gradient âˆ‚h/âˆ‚q = {grad_q.item()}\")\n",
    "create_check_button(\"exercise_6\", lambda: validate_exercise_6(p, q, h, grad_p, grad_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2aa9e",
   "metadata": {},
   "source": [
    "## Summary: PyTorch NN Layers\n",
    "\n",
    "### Linear\n",
    "- `nn.Linear(in, out)` - Fully connected\n",
    "- `nn.Bilinear()` - Bilinear transformation\n",
    "\n",
    "### Convolutional\n",
    "- `nn.Conv1d/2d/3d()` - 1D/2D/3D convolution\n",
    "- `nn.ConvTranspose2d()` - Upsampling\n",
    "\n",
    "### Pooling\n",
    "- `nn.MaxPool2d()` - Max pooling\n",
    "- `nn.AvgPool2d()` - Average pooling\n",
    "- `nn.AdaptiveAvgPool2d()` - Adaptive pooling\n",
    "\n",
    "### Activation\n",
    "- `nn.ReLU()`, `nn.LeakyReLU()`, `nn.GELU()`\n",
    "- `nn.Sigmoid()`, `nn.Tanh()`, `nn.Softmax()`\n",
    "\n",
    "### Normalization\n",
    "- `nn.BatchNorm2d()` - Batch normalization\n",
    "- `nn.LayerNorm()` - Layer normalization\n",
    "- `nn.GroupNorm()` - Group normalization\n",
    "\n",
    "### Recurrent\n",
    "- `nn.RNN()`, `nn.LSTM()`, `nn.GRU()`\n",
    "\n",
    "### Transformer\n",
    "- `nn.Transformer()` - Full transformer\n",
    "- `nn.TransformerEncoder/Decoder()`\n",
    "- `nn.MultiheadAttention()`\n",
    "\n",
    "### Regularization\n",
    "- `nn.Dropout()`, `nn.Dropout2d()`\n",
    "\n",
    "### Embedding\n",
    "- `nn.Embedding()` - Lookup table\n",
    "\n",
    "### Loss Functions\n",
    "- `nn.CrossEntropyLoss()` - Classification\n",
    "- `nn.MSELoss()` - Regression\n",
    "- `nn.BCEWithLogitsLoss()` - Binary classification\n",
    "\n",
    "### Utility\n",
    "- `nn.Sequential()` - Chain layers\n",
    "- `nn.ModuleList/Dict()` - Dynamic layers\n",
    "- `nn.Flatten()` - Flatten dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f125d7e",
   "metadata": {},
   "source": [
    "### Example: Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SimpleCNN()\n",
    "print(model)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc229e6",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 7: Neural Network Forward Pass\n",
    "\n",
    "**Your Task:** Implement a forward pass through a neural network layer with autograd.\n",
    "\n",
    "**Instructions:**\n",
    "1. Create weight matrix `W` (2x3) and bias vector `b` (size 3) with random values and `requires_grad=True`\n",
    "2. Create input tensor `x_input` (size 2)\n",
    "3. Compute the forward pass: `z = W.T @ x_input + b`\n",
    "4. Apply sigmoid activation: `output = torch.sigmoid(z)`\n",
    "5. Compute loss as sum of outputs and call `.backward()`\n",
    "\n",
    "After this, gradients will be available in `W.grad` and `b.grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061499ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the blanks below\n",
    "\n",
    "# Create weight matrix W (2x3) with requires_grad=True\n",
    "W = None  # torch.randn(2, 3, requires_grad=True)\n",
    "\n",
    "# Create bias vector b (size 3) with requires_grad=True\n",
    "b = None  # torch.randn(3, requires_grad=True)\n",
    "\n",
    "# Create input tensor x_input (size 2)\n",
    "x_input = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Forward pass: z = W.T @ x_input + b\n",
    "z = None  # Complete the forward pass\n",
    "\n",
    "# Apply sigmoid activation\n",
    "output = None  # torch.sigmoid(z)\n",
    "\n",
    "# Compute loss as sum of outputs\n",
    "loss = None  # output.sum()\n",
    "\n",
    "# Compute gradients - call backward on loss\n",
    "# YOUR CODE HERE to call .backward()\n",
    "\n",
    "# Display results\n",
    "if W is not None and b is not None:\n",
    "    print(\"Weight matrix W shape:\", W.shape)\n",
    "    print(\"Bias vector b shape:\", b.shape)\n",
    "if output is not None:\n",
    "    print(\"Output:\", output)\n",
    "if loss is not None:\n",
    "    print(\"Loss:\", loss.item())\n",
    "if W is not None and W.grad is not None:\n",
    "    print(\"W.grad shape:\", W.grad.shape)\n",
    "if b is not None and b.grad is not None:\n",
    "    print(\"b.grad shape:\", b.grad.shape)\n",
    "create_check_button(\"exercise_7\", lambda: validate_exercise_7(W, b, z, output, loss, x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12393453",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True)\n",
    "image, label = image_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique run directory with timestamp to avoid multiple graph events\n",
    "run_dir = f'runs/simple_cnn_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "writer = SummaryWriter(run_dir)\n",
    "print(f\"TensorBoard logs saved to: {run_dir}\")\n",
    "\n",
    "# Convert CIFAR10 image to correct PyTorch format\n",
    "# CIFAR10 images are (H, W, C) format, but PyTorch CNNs need (B, C, H, W)\n",
    "x = np.asarray(image)  # Shape: (32, 32, 3)\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "\n",
    "# Use .permute() to rearrange dimensions from (H, W, C) to (C, H, W)\n",
    "x = x.permute(2, 0, 1)  # Now shape: (3, 32, 32)\n",
    "print(f\"After permute: {x.shape}\")\n",
    "\n",
    "# Add batch dimension\n",
    "x = x.unsqueeze(0)  # Now shape: (1, 3, 32, 32)\n",
    "x.requires_grad = True\n",
    "print(f\"Final shape: {x.shape}\")\n",
    "\n",
    "# Add graph only once\n",
    "writer.add_graph(model, x)\n",
    "writer.close()\n",
    "print(\"CNN graph added successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8dee5b",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Exercise 8: Build a Custom Neural Network\n",
    "\n",
    "**Your Task:** Complete a simple multi-layer perceptron (MLP) class.\n",
    "\n",
    "**Instructions:**\n",
    "Complete the `SimpleClassifier` class below by:\n",
    "1. Adding a second linear layer in `__init__`: `self.fc2 = nn.Linear(64, 10)`\n",
    "2. Implementing the forward pass:\n",
    "   - Apply first linear layer\n",
    "   - Apply ReLU activation\n",
    "   - Apply dropout\n",
    "   - Apply second linear layer\n",
    "   - Return the output (logits)\n",
    "\n",
    "The model should take input of size 128 and output 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9866a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the SimpleClassifier class below\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        # First linear layer (already provided)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # TODO: Add second linear layer\n",
    "        # self.fc2 = ???\n",
    "        \n",
    "        # Activation and regularization (already provided)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        TODO: Complete this method\n",
    "        \"\"\"\n",
    "        # TODO: Apply fc1\n",
    "        # x = ???\n",
    "        \n",
    "        # TODO: Apply ReLU activation\n",
    "        # x = ???\n",
    "        \n",
    "        # TODO: Apply dropout\n",
    "        # x = ???\n",
    "        \n",
    "        # TODO: Apply fc2 to get logits\n",
    "        # x = ???\n",
    "        \n",
    "        return None  # Replace None with x\n",
    "\n",
    "# Test your model\n",
    "test_model = SimpleClassifier()\n",
    "test_input = torch.randn(4, 128)  # Batch of 4 samples, each with 128 features\n",
    "\n",
    "print(\"Model created!\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "\n",
    "# Try forward pass\n",
    "test_output = test_model(test_input)\n",
    "if test_output is not None:\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {test_output.shape}\")\n",
    "create_check_button(\"exercise_8\", lambda: validate_exercise_8(SimpleClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377f0fd",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ“ Exercise Summary\n",
    "\n",
    "Congratulations on completing the PyTorch Tensors and Autograd exercises!\n",
    "\n",
    "### What You've Learned:\n",
    "1. âœ“ **Creating Tensors** - zeros, ones, random tensors\n",
    "2. âœ“ **NumPy Conversion** - converting between NumPy and PyTorch\n",
    "3. âœ“ **Tensor Operations** - element-wise, matrix multiplication, transpose\n",
    "4. âœ“ **Reshaping** - reshape, view, flatten\n",
    "5. âœ“ **Manual Gradients** - calculating derivatives by hand\n",
    "6. âœ“ **PyTorch Autograd** - automatic differentiation\n",
    "7. âœ“ **Neural Network Layers** - forward and backward passes\n",
    "8. âœ“ **Custom Models** - building your own neural network\n",
    "\n",
    "### Next Steps:\n",
    "- Explore the other notebooks in this tutorial series\n",
    "- Build more complex neural networks\n",
    "- Try training on real datasets\n",
    "- Experiment with different architectures\n",
    "\n",
    "**Keep practicing and happy learning!** ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
